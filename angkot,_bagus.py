# -*- coding: utf-8 -*-
"""Angkot, bagus

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wbfJkAglnDeQ5bG4koydCaYCiSsnnfJM
"""

!pip install google-play-scraper

from google_play_scraper import app
import pandas as pd
import numpy as np

from google_play_scraper import Sort, reviews

result, continuation_token =reviews(
    'codexplore.angkot',
    lang='id',
    country='id',
    sort= Sort.MOST_RELEVANT,
    count=1000,
    filter_score_with=None
)

df_busu = pd.DataFrame(np.array(result),columns=['review'])

df_busu = df_busu.join(pd.DataFrame(df_busu.pop('review').tolist()))

df_busu.head()

len(df_busu.index)

df_busu[['userName','at','content','score']].head()

new_df = df_busu[['userName','at','content','score']]
sorted_df = new_df.sort_values(by='at',ascending=False)
sorted_df.head()

new_df.to_csv("angkothas_scrap.csv", index= False)

# Load the new CSV file to inspect its structure and clean it
file_path_new = '/content/angkothas_scrap.csv'
df_new = pd.read_csv(file_path_new)

# Display the first few rows to assess its structure and check column names
df_new.head(), df_new.columns


# Remove leading/trailing whitespace from column names
new_df.columns = new_df.columns.str.strip()

# Check for and remove duplicate rows
new_df = new_df.drop_duplicates()

# Save the cleaned DataFrame to an Excel file
excel_file_path_new = '/content/angkothas_scrap_cleaned.xlsx'
new_df.to_excel(excel_file_path_new, index=False)

# Provide the download link for the user
excel_file_path_new

angkot_df =sorted_df[['content','score']]
angkot_df.head()

"""**PELABELAN**"""

def pelabelan(score):
  if score <3:
    return 'Negatif'
  elif score == 4:
    return 'Positif'
  elif score ==5:
    return 'Positif'
angkot_df['label']=angkot_df['score'].apply(pelabelan)
angkot_df.head(100)

angkot_df.dropna(subset=['label'],inplace=True)
angkot_df.isnull().sum()

angkot_df.head(100)

len(angkot_df)

angkot_df = angkot_df.drop(columns=['score'])
angkot_df

print(angkot_df.isnull().sum())

angkot_df.to_csv("angkot1_scrap.csv", index= False)

import pandas as pd
data= pd.read_csv('/content/angkot1_scrap.csv')
data.head()

"""INSTAL PACKAGE"""

!pip install Wordcloud
!pip install Sastrawi

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import re
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report
from sklearn.naive_bayes import MultinomialNB

from wordcloud import WordCloud, STOPWORDS

import pandas as pd
import re
import string
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import nltk

import pandas as pd
import re
import string
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import nltk

# Load data
data = pd.read_csv('angkot1_scrap.csv')

# Download necessary NLTK stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('indonesian'))

# Initialize stemmer for Indonesian language
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Text preprocessing function
def preprocess_text(text):
    # 1. Case folding
    text = text.lower()
    # 2. Normalization (basic - add more terms if needed)
    normalization_dict = {'gak': 'tidak', 'ga': 'tidak', 'yg': 'yang', 'bg': 'bagus','bengsin':'bensin','geme':'game','tlong':'tolong','abes':'habis','abdet':'update'}  # Add more as necessary
    text = ' '.join([normalization_dict[word] if word in normalization_dict else word for word in text.split()])

    # Menghilangkan angka
    text = re.sub(r'\d+', '', text)

    # 3. Stopword removal

    # Update stop words with custom words
    stop_words.update(['nya', 'pas', 'kalo'])

    # Stop word removal process
    text = ' '.join([word for word in text.split() if word not in stop_words and word not in ['g', 'ny', ' boong', 'tp','bngt','ya','knp','sih','lah','cuman','nya','bgt','udah','dah','banget','dari','assalamualaikum','to','ada','setansifer','zuuk','zxr','yudistira','yudhistira']])

    # 4. Punctuation removal
    text = text.translate(str.maketrans('', '', string.punctuation))

    # 5. Tokenization is implied in the stopword and stemming processes
    # 6. Stemming
    text = ' '.join([stemmer.stem(word) for word in text.split()])
    return text

# Apply preprocessing
data['cleaned_content'] = data['content'].apply(preprocess_text)

# Display preprocessed data
print(data[['content', 'cleaned_content', 'label']].head())

# Step 7: Save cleaned data
data.to_csv('angkot1_scrap_cleaned.csv', index=False)

from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Hitung frekuensi kata
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(data['cleaned_content'])
word_counts = pd.DataFrame({'word': vectorizer.get_feature_names_out(), 'count': X.sum(axis=0).A1})

# Temukan kata-kata dengan frekuensi rendah
rare_words = word_counts[word_counts['count'] < 3]  # Bisa disesuaikan sesuai kebutuhan
rare_words_list = rare_words['word'].tolist()

# Menghapus kata-kata dengan frekuensi rendah dari teks
def remove_rare_words(text):
    return ' '.join([word for word in text.split() if word not in rare_words_list])

# Terapkan ke kolom teks
data['cleaned_content'] = data['cleaned_content'].apply(remove_rare_words)

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

# Assume 'data' is your DataFrame with 'cleaned_content' and 'label'
tfidf_vectorizer = TfidfVectorizer()
X = tfidf_vectorizer.fit_transform(data['cleaned_content'])  # Transform the preprocessed text data
y = data['label']  # The labels ('Positif' or 'Negatif')

# Create a DataFrame from the TF-IDF matrix
tfidf_df = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Add the labels to the DataFrame for better understanding
tfidf_df['label'] = y.values  # Add the labels column

# Display the entire DataFrame (can be large)
print(tfidf_df)

# Alternatively, if the DataFrame is too large, you can save it to a CSV file:
# tfidf_df.to_csv('tfidf_results.csv', index=False)

tfidf_df.to_csv('tfidf_results.csv', index=False)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Separate positive and negative reviews
positive_text = ' '.join(data[data['label'] == 'Positif']['cleaned_content'])
negative_text = ' '.join(data[data['label'] == 'Negatif']['cleaned_content'])

# Generate word clouds
wordcloud_positive = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(positive_text)
wordcloud_negative = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(negative_text)

# Plotting the word clouds
plt.figure(figsize=(15, 7))

# Positive Word Cloud
plt.subplot(1, 2, 1)
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis('off')
plt.title('Positive Reviews')

# Negative Word Cloud
plt.subplot(1, 2, 2)
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis('off')
plt.title('Negative Reviews')

plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Count the number of positive and negative labels
label_counts = data['label'].value_counts()

# Plot the distribution
plt.figure(figsize=(8, 5))
sns.barplot(x=label_counts.index, y=label_counts.values, palette=['red', 'green'])
plt.title('Distribution of Positive and Negative Reviews')
plt.xlabel('Review Sentiment')
plt.ylabel('Count')
plt.show()

from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to the TF-IDF features and labels
X_balanced, y_balanced = smote.fit_resample(X, y)

# Check the new balance
print("Balanced label counts:")
print(pd.Series(y_balanced).value_counts())

#MEMUNCULKAN PERBANDINGAN SEBELUM DAN SETELAH SMOTE

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# Hitung distribusi label sebelum SMOTE
original_counts = Counter(y)

# Hitung distribusi label setelah SMOTE
balanced_counts = Counter(y_balanced)

# Plot distribusi sebelum dan sesudah SMOTE
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# Sebelum SMOTE
sns.barplot(x=list(original_counts.keys()), y=list(original_counts.values()), ax=ax[0], palette="Blues")
ax[0].set_title('Distribusi Label Sebelum SMOTE')
ax[0].set_xlabel('Label')
ax[0].set_ylabel('Jumlah')

# Setelah SMOTE
sns.barplot(x=list(balanced_counts.keys()), y=list(balanced_counts.values()), ax=ax[1], palette="Greens")
ax[1].set_title('Distribusi Label Setelah SMOTE')
ax[1].set_xlabel('Label')
ax[1].set_ylabel('Jumlah')

plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Split the balanced dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Initialize the Naive Bayes classifier
nb_classifier = MultinomialNB()

# Train the model
nb_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = nb_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

# Visualize confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negatif', 'Positif'], yticklabels=['Negatif', 'Positif'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Classification report for detailed metrics
print("Classification Report:\n", classification_report(y_test, y_pred))

#TF IDF

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
#Proses TF-IDF (memberikan bobot pada kata)
tf = TfidfVectorizer()
text_tf = tf.fit_transform(data['content'].astype('U'))
text_tf
print(text_tf)